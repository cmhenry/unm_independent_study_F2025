{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Python Text Analysis Essentials for Political Science"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìù Notebook 2: Python Essentials for Text Analysis\n",
        "## Processing Political & Legal Documents with Python\n",
        "\n",
        "**Time to complete:** 90-120 minutes  \n",
        "**Prerequisites:** Completed Notebook 1 (R-to-Python Translation)  \n",
        "**What you'll build:** A complete text processing pipeline for your dissertation data!\n",
        "\n",
        "### üéØ Learning Objectives\n",
        "1. Load and explore text data from various sources\n",
        "2. Clean and preprocess political/legal documents\n",
        "3. Extract basic features and patterns\n",
        "4. Save processed data for analysis\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Part 1: Setup and Installation (One Click!)\n",
        "\n",
        "Run this cell once at the beginning of each session:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install necessary packages (only needed once per session in Colab)\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    print(\"üîß Installing packages for Google Colab...\")\n",
        "    !pip install -q wordcloud\n",
        "    print(\"‚úÖ All packages installed!\")\n",
        "else:\n",
        "    print(\"üìç Not in Colab - assuming packages are installed\")\n",
        "\n",
        "# Import everything we need\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "import textwrap\n",
        "\n",
        "print(\"‚úÖ Setup complete! You're ready to analyze text.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ Part 2: Loading Your Text Data\n",
        "\n",
        "We'll practice with three common data formats you'll encounter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example 1: Create sample political speech data\n",
        "speeches_data = {\n",
        "    'speaker': ['Biden', 'Trump', 'Sanders', 'Harris', 'DeSantis'],\n",
        "    'date': ['2024-01-15', '2024-02-20', '2024-01-28', '2024-03-10', '2024-02-05'],\n",
        "    'text': [\n",
        "        \"\"\"My fellow Americans, we stand at an inflection point in history. \n",
        "        Together, we can build a better future for our children. The work ahead \n",
        "        will not be easy, but I know the American people are up to the task.\"\"\",\n",
        "        \n",
        "        \"\"\"This administration is a disaster, folks. They're destroying our \n",
        "        country. We had the greatest economy in history, and they ruined it. \n",
        "        We need to make America great again, and we will!\"\"\",\n",
        "        \n",
        "        \"\"\"The billionaire class has rigged our economy. Working families \n",
        "        deserve a living wage, healthcare as a human right, and free public \n",
        "        college. We need a political revolution in this country.\"\"\",\n",
        "        \n",
        "        \"\"\"We must protect our democracy and our fundamental rights. Women's \n",
        "        rights are human rights. We will not go back. Together, we will \n",
        "        move forward and build a more just society.\"\"\",\n",
        "        \n",
        "        \"\"\"The woke ideology is poisoning our schools and destroying our values. \n",
        "        We must fight back against this radical agenda. Florida is where woke \n",
        "        goes to die, and we're just getting started.\"\"\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "speeches_df = pd.DataFrame(speeches_data)\n",
        "print(\"üìä Loaded political speeches dataset:\")\n",
        "print(speeches_df[['speaker', 'date']].head())\n",
        "print(f\"\\nüìù Total speeches: {len(speeches_df)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example 2: Create sample legal document data\n",
        "legal_data = {\n",
        "    'document_id': ['RON_001', 'RON_002', 'RON_003'],\n",
        "    'jurisdiction': ['Boulder, CO', 'Toledo, OH', 'Orange County, FL'],\n",
        "    'year': [2021, 2022, 2023],\n",
        "    'text': [\n",
        "        \"\"\"Whereas natural ecosystems have inherent rights to exist and flourish, \n",
        "        the Boulder Creek watershed shall possess legal standing. Any citizen may \n",
        "        bring action to enforce these rights in court.\"\"\",\n",
        "        \n",
        "        \"\"\"Lake Erie Bill of Rights: Lake Erie and its watershed possess the right \n",
        "        to exist, flourish, and naturally evolve. The people of Toledo have the \n",
        "        right to a clean and healthy environment.\"\"\",\n",
        "        \n",
        "        \"\"\"The Wekiva River and Econlockhatchee River possess fundamental rights \n",
        "        to flow, to be free from pollution, and to maintain their natural water levels. \n",
        "        These rights shall be enforced by county residents.\"\"\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "legal_df = pd.DataFrame(legal_data)\n",
        "print(\"\\nüìú Loaded legal documents dataset:\")\n",
        "print(legal_df[['document_id', 'jurisdiction']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Loading Your Own Data\n",
        "\n",
        "**For Fairooz (Political Speeches):**\n",
        "Upload your CSV with speeches to Colab, then uncomment and run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === UNCOMMENT AND MODIFY FOR YOUR DATA ===\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()  # Click to upload your file\n",
        "\n",
        "# # Load your political speeches CSV\n",
        "# your_speeches = pd.read_csv('your_speeches.csv')  # Change filename\n",
        "# print(your_speeches.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**For Brisa (Legal Documents):**\n",
        "Upload your CSV with RoN documents to Colab, then uncomment and run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === UNCOMMENT AND MODIFY FOR YOUR DATA ===\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()  # Click to upload your file\n",
        "\n",
        "# # Load your legal documents CSV\n",
        "# your_legal_docs = pd.read_csv('your_legal_docs.csv')  # Change filename\n",
        "# print(your_legal_docs.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßπ Part 3: Text Cleaning Pipeline\n",
        "\n",
        "Let's build a reusable cleaning function for political/legal text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def clean_text_basic(text):\n",
        "    \"\"\"\n",
        "    Basic text cleaning for political/legal documents\n",
        "    Keep this simple - we'll add complexity later!\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    # Convert to string and lowercase\n",
        "    text = str(text).lower()\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    \n",
        "    # Remove special characters but keep sentence structure\n",
        "    text = re.sub(r'[^\\w\\s.!?]', '', text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Test the function\n",
        "sample_text = speeches_df['text'][0]\n",
        "cleaned = clean_text_basic(sample_text)\n",
        "\n",
        "print(\"üî§ Original text (first 100 chars):\")\n",
        "print(textwrap.fill(sample_text[:100], width=60))\n",
        "print(\"\\nüßπ Cleaned text (first 100 chars):\")\n",
        "print(textwrap.fill(cleaned[:100], width=60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply cleaning to all speeches\n",
        "speeches_df['cleaned_text'] = speeches_df['text'].apply(clean_text_basic)\n",
        "legal_df['cleaned_text'] = legal_df['text'].apply(clean_text_basic)\n",
        "\n",
        "print(\"‚úÖ Text cleaning complete!\")\n",
        "print(f\"Speeches cleaned: {len(speeches_df)}\")\n",
        "print(f\"Legal docs cleaned: {len(legal_df)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Part 4: Basic Text Analysis\n",
        "\n",
        "Let's extract useful features from our text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def analyze_text(text):\n",
        "    \"\"\"\n",
        "    Extract basic statistics from text\n",
        "    Similar to what you might do with stringr in R\n",
        "    \"\"\"\n",
        "    # Word count\n",
        "    words = text.split()\n",
        "    word_count = len(words)\n",
        "    \n",
        "    # Sentence count (approximate)\n",
        "    sentences = re.findall(r'[.!?]+', text)\n",
        "    sentence_count = len(sentences) if sentences else 1\n",
        "    \n",
        "    # Average words per sentence\n",
        "    avg_words_per_sentence = word_count / sentence_count if sentence_count > 0 else 0\n",
        "    \n",
        "    # Unique words (vocabulary richness)\n",
        "    unique_words = len(set(words))\n",
        "    \n",
        "    # Lexical diversity\n",
        "    lexical_diversity = unique_words / word_count if word_count > 0 else 0\n",
        "    \n",
        "    return {\n",
        "        'word_count': word_count,\n",
        "        'sentence_count': sentence_count,\n",
        "        'avg_words_per_sentence': round(avg_words_per_sentence, 1),\n",
        "        'unique_words': unique_words,\n",
        "        'lexical_diversity': round(lexical_diversity, 3)\n",
        "    }\n",
        "\n",
        "# Test on one speech\n",
        "sample_analysis = analyze_text(speeches_df['cleaned_text'][0])\n",
        "print(\"üìà Text analysis for Biden's speech:\")\n",
        "for key, value in sample_analysis.items():\n",
        "    print(f\"  {key}: {value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply analysis to all documents\n",
        "# This is like using mutate() in R to add multiple columns\n",
        "\n",
        "# For speeches\n",
        "for index, row in speeches_df.iterrows():\n",
        "    stats = analyze_text(row['cleaned_text'])\n",
        "    for key, value in stats.items():\n",
        "        speeches_df.at[index, key] = value\n",
        "\n",
        "print(\"üìä Speech Statistics:\")\n",
        "print(speeches_df[['speaker', 'word_count', 'avg_words_per_sentence', 'lexical_diversity']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick visualization of results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Word count by speaker\n",
        "axes[0].bar(speeches_df['speaker'], speeches_df['word_count'], color='steelblue')\n",
        "axes[0].set_title('Word Count by Speaker')\n",
        "axes[0].set_xlabel('Speaker')\n",
        "axes[0].set_ylabel('Words')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Lexical diversity\n",
        "axes[1].bar(speeches_df['speaker'], speeches_df['lexical_diversity'], color='coral')\n",
        "axes[1].set_title('Lexical Diversity by Speaker')\n",
        "axes[1].set_xlabel('Speaker')\n",
        "axes[1].set_ylabel('Diversity Score')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Part 5: Pattern Detection for Political/Legal Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define patterns relevant to political science research\n",
        "\n",
        "# For Fairooz: Detecting rhetorical patterns\n",
        "political_patterns = {\n",
        "    'unity_language': ['together', 'united', 'unity', 'we', 'us', 'our'],\n",
        "    'crisis_language': ['crisis', 'disaster', 'emergency', 'threat', 'danger'],\n",
        "    'populist_language': ['elite', 'rigged', 'establishment', 'people', 'corrupt'],\n",
        "    'rights_language': ['rights', 'freedom', 'liberty', 'justice', 'democracy']\n",
        "}\n",
        "\n",
        "# For Brisa: Legal document patterns  \n",
        "legal_patterns = {\n",
        "    'rights_granted': ['right to', 'shall possess', 'entitled to', 'authority to'],\n",
        "    'enforcement': ['enforce', 'action', 'court', 'standing', 'bring suit'],\n",
        "    'environmental': ['ecosystem', 'watershed', 'natural', 'pollution', 'environment'],\n",
        "    'procedural': ['whereas', 'shall', 'pursuant', 'herein', 'thereof']\n",
        "}\n",
        "\n",
        "def count_patterns(text, patterns_dict):\n",
        "    \"\"\"\n",
        "    Count occurrences of pattern categories in text\n",
        "    \"\"\"\n",
        "    text_lower = text.lower()\n",
        "    results = {}\n",
        "    \n",
        "    for category, patterns in patterns_dict.items():\n",
        "        count = sum(1 for pattern in patterns if pattern in text_lower)\n",
        "        results[category] = count\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Test on a speech\n",
        "biden_patterns = count_patterns(speeches_df['text'][0], political_patterns)\n",
        "print(\"üéØ Patterns in Biden's speech:\")\n",
        "for pattern, count in biden_patterns.items():\n",
        "    print(f\"  {pattern}: {count}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply pattern detection to all speeches\n",
        "for pattern_type in political_patterns.keys():\n",
        "    speeches_df[pattern_type] = speeches_df['text'].apply(\n",
        "        lambda x: count_patterns(x, political_patterns)[pattern_type]\n",
        "    )\n",
        "\n",
        "print(\"\\nüìä Pattern Analysis Results:\")\n",
        "print(speeches_df[['speaker'] + list(political_patterns.keys())])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üè∑Ô∏è Part 6: Keyword Extraction\n",
        "\n",
        "Extract the most important words from each document:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def extract_keywords(text, num_keywords=10, stop_words=None):\n",
        "    \"\"\"\n",
        "    Extract most common meaningful words from text\n",
        "    Similar to what you might do with tm package in R\n",
        "    \"\"\"\n",
        "    # Default stop words (common words to ignore)\n",
        "    if stop_words is None:\n",
        "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
        "                     'of', 'with', 'by', 'from', 'is', 'was', 'are', 'were', 'be', 'been',\n",
        "                     'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',\n",
        "                     'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that',\n",
        "                     'these', 'those', 'i', 'you', 'we', 'they', 'it', 'he', 'she'}\n",
        "    \n",
        "    # Tokenize and clean\n",
        "    words = text.lower().split()\n",
        "    words = [w for w in words if w.isalpha() and w not in stop_words and len(w) > 3]\n",
        "    \n",
        "    # Count frequencies\n",
        "    word_freq = Counter(words)\n",
        "    \n",
        "    # Get top keywords\n",
        "    top_keywords = word_freq.most_common(num_keywords)\n",
        "    \n",
        "    return top_keywords\n",
        "\n",
        "# Extract keywords for each speaker\n",
        "print(\"üîë Top Keywords by Speaker:\\n\")\n",
        "for idx, row in speeches_df.iterrows():\n",
        "    keywords = extract_keywords(row['text'], num_keywords=5)\n",
        "    print(f\"{row['speaker']}:\")\n",
        "    for word, freq in keywords:\n",
        "        print(f\"  ‚Ä¢ {word} ({freq})\")\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Part 7: Saving Your Processed Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare final dataset with all our extracted features\n",
        "final_speeches = speeches_df[['speaker', 'date', 'cleaned_text', \n",
        "                              'word_count', 'sentence_count',\n",
        "                              'avg_words_per_sentence', 'lexical_diversity',\n",
        "                              'unity_language', 'crisis_language', \n",
        "                              'populist_language', 'rights_language']]\n",
        "\n",
        "# Save to CSV (can open in R!)\n",
        "output_filename = 'processed_speeches.csv'\n",
        "final_speeches.to_csv(output_filename, index=False)\n",
        "print(f\"‚úÖ Saved processed data to '{output_filename}'\")\n",
        "print(\"\\nüìä Data preview:\")\n",
        "print(final_speeches.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Also save summary statistics\n",
        "summary_stats = speeches_df.groupby('speaker').agg({\n",
        "    'word_count': 'mean',\n",
        "    'lexical_diversity': 'mean',\n",
        "    'unity_language': 'sum',\n",
        "    'crisis_language': 'sum',\n",
        "    'populist_language': 'sum',\n",
        "    'rights_language': 'sum'\n",
        "}).round(2)\n",
        "\n",
        "summary_stats.to_csv('speech_summary_stats.csv')\n",
        "print(\"\\nüìà Summary statistics by speaker:\")\n",
        "print(summary_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Part 8: Your Turn - Process Your Own Data!\n",
        "\n",
        "Now apply everything to your dissertation data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === TEMPLATE FOR YOUR DATA ===\n",
        "# Modify this code block for your specific needs\n",
        "\n",
        "def process_my_documents(df, text_column='text'):\n",
        "    \"\"\"\n",
        "    Complete pipeline for processing political/legal documents\n",
        "    \n",
        "    Parameters:\n",
        "    df: Your dataframe\n",
        "    text_column: Name of the column containing text\n",
        "    \"\"\"\n",
        "    # 1. Clean text\n",
        "    df['cleaned_text'] = df[text_column].apply(clean_text_basic)\n",
        "    \n",
        "    # 2. Basic statistics\n",
        "    for index, row in df.iterrows():\n",
        "        stats = analyze_text(row['cleaned_text'])\n",
        "        for key, value in stats.items():\n",
        "            df.at[index, key] = value\n",
        "    \n",
        "    # 3. Pattern detection (customize patterns for your research!)\n",
        "    # === MODIFY THIS SECTION ===\n",
        "    my_patterns = {\n",
        "        'pattern1': ['word1', 'word2'],  # Add your keywords\n",
        "        'pattern2': ['word3', 'word4'],  # Add more categories\n",
        "    }\n",
        "    # === END MODIFICATION ===\n",
        "    \n",
        "    for pattern_type in my_patterns.keys():\n",
        "        df[pattern_type] = df['cleaned_text'].apply(\n",
        "            lambda x: count_patterns(x, my_patterns)[pattern_type]\n",
        "        )\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Example usage (uncomment when you have your data):\n",
        "# my_processed_data = process_my_documents(your_data, text_column='your_text_column')\n",
        "# my_processed_data.to_csv('my_processed_data.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Part 9: Quick Reference Functions\n",
        "\n",
        "Here are all the functions we created, ready to copy and use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === SAVE THIS CELL FOR FUTURE USE ===\n",
        "\n",
        "def text_processing_toolkit():\n",
        "    \"\"\"\n",
        "    All your text processing functions in one place\n",
        "    Copy this to any new notebook!\n",
        "    \"\"\"\n",
        "    \n",
        "    functions = {\n",
        "        'clean_text': clean_text_basic,\n",
        "        'analyze_text': analyze_text,\n",
        "        'count_patterns': count_patterns,\n",
        "        'extract_keywords': extract_keywords,\n",
        "        'process_documents': process_my_documents\n",
        "    }\n",
        "    \n",
        "    return functions\n",
        "\n",
        "# Get your toolkit\n",
        "toolkit = text_processing_toolkit()\n",
        "print(\"üõ†Ô∏è Your text processing toolkit contains:\")\n",
        "for name in toolkit.keys():\n",
        "    print(f\"  ‚Ä¢ {name}()\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèÅ Part 10: Next Steps and Resources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a word cloud visualization (fun bonus!)\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all speeches\n",
        "all_text = ' '.join(speeches_df['cleaned_text'])\n",
        "\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, \n",
        "                      background_color='white',\n",
        "                      colormap='viridis').generate(all_text)\n",
        "\n",
        "# Display\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Most Common Words in Political Speeches')\n",
        "plt.show()\n",
        "\n",
        "print(\"üé® Word cloud generated from your corpus!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Congratulations! You've Completed Text Analysis Essentials!\n",
        "\n",
        "### üìä What You've Accomplished:\n",
        "- ‚úÖ Loaded and explored text data\n",
        "- ‚úÖ Built a complete text cleaning pipeline\n",
        "- ‚úÖ Extracted meaningful features and patterns\n",
        "- ‚úÖ Analyzed political/legal language patterns\n",
        "- ‚úÖ Created reusable functions for your research\n",
        "- ‚úÖ Saved processed data for further analysis\n",
        "\n",
        "### üéØ Your Homework:\n",
        "1. **Upload your dissertation data** to Colab\n",
        "2. **Process at least 20 documents** using the pipeline\n",
        "3. **Identify 3 interesting patterns** in your data\n",
        "4. **Save your processed dataset** for next week\n",
        "\n",
        "### üìö Resources for This Week:\n",
        "\n",
        "**Documentation:**\n",
        "- [Pandas Text Methods](https://pandas.pydata.org/docs/user_guide/text.html)\n",
        "- [Python Regular Expressions](https://docs.python.org/3/howto/regex.html)\n",
        "- [Google Colab Tips](https://colab.research.google.com/notebooks/snippets/importing_libraries.ipynb)\n",
        "\n",
        "**For R Users:**\n",
        "- Your processed CSV can be loaded directly into R!\n",
        "- Use `write.csv()` in Python ‚Üí `read.csv()` in R\n",
        "- All numeric features we created work perfectly in R's statistical functions\n",
        "\n",
        "### üí¨ Getting Help:\n",
        "```python\n",
        "# If you get stuck, try these:\n",
        "\n",
        "# 1. Check data types\n",
        "df.dtypes\n",
        "\n",
        "# 2. Check for missing values  \n",
        "df.isna().sum()\n",
        "\n",
        "# 3. Preview your data\n",
        "df.head()\n",
        "\n",
        "# 4. Check shape\n",
        "df.shape\n",
        "```\n",
        "\n",
        "### üöÄ Ready for Week 1?\n",
        "Next week we'll use spaCy for advanced NLP - but you now have all the Python basics you need!\n",
        "\n",
        "**Remember:**\n",
        "- Your R knowledge is valuable - you're just learning new syntax\n",
        "- Focus on your research questions, not perfect code\n",
        "- These notebooks are yours to modify and reuse\n",
        "- Help is always available in Slack!\n",
        "\n",
        "---\n",
        "\n",
        "**üìù Note:** Save this notebook! You'll reference these functions throughout October."
      ]
    }
  ]
}