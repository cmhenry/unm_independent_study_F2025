setwd("~/Projects/unm_independent_study_F2025/week1")
library(dplyr)
library(readr)
# Sample texts inspired by actual political discourse
democrat_economy <- c(
"We must invest in infrastructure and create millions of good-paying jobs for working families across America",
"Economic recovery requires supporting small businesses and ensuring fair wages for all workers",
"We need to strengthen the middle class through tax fairness and economic opportunity",
"Creating clean energy jobs will drive economic growth while protecting our planet",
"Healthcare costs are crushing families and we must make quality care affordable for everyone"
)
democrat_social <- c(
"Protecting voting rights is essential to preserving our democratic institutions",
"We must defend civil rights and fight discrimination in all its forms",
"Education is the key to opportunity and we need to invest in our schools and teachers",
"Immigration reform should reflect our values of compassion while maintaining security",
"Women's reproductive rights must be protected as a fundamental freedom"
)
republican_economy <- c(
"Lower taxes and reduced regulations will unleash economic growth and job creation",
"Free market solutions drive innovation and prosperity better than government intervention",
"We must reduce the national debt and ensure fiscal responsibility for future generations",
"Energy independence through domestic production will strengthen our economy and security",
"Small government and individual liberty are the foundations of American prosperity"
)
republican_security <- c(
"National security must remain our top priority in an increasingly dangerous world",
"We need strong borders to protect American citizens and maintain law and order",
"Supporting our military and veterans is essential to defending freedom",
"Law and order must be restored in our communities through supporting police",
"Second Amendment rights are fundamental to American liberty and must be protected"
)
bipartisan_texts <- c(
"We must work together across party lines to address the opioid crisis",
"Infrastructure investment will create jobs and improve quality of life nationwide",
"Veterans deserve our full support and quality healthcare services",
"American innovation and competitiveness depend on STEM education",
"Rural communities need better broadband access for economic development"
)
# Create balanced dataset
set.seed(123)
# Calculate how many of each type
n_dem_econ <- round(n_docs * 0.20)
generate_sample_corpus <- function(n_docs = 60) {
# Sample texts inspired by actual political discourse
democrat_economy <- c(
"We must invest in infrastructure and create millions of good-paying jobs for working families across America",
"Economic recovery requires supporting small businesses and ensuring fair wages for all workers",
"We need to strengthen the middle class through tax fairness and economic opportunity",
"Creating clean energy jobs will drive economic growth while protecting our planet",
"Healthcare costs are crushing families and we must make quality care affordable for everyone"
)
democrat_social <- c(
"Protecting voting rights is essential to preserving our democratic institutions",
"We must defend civil rights and fight discrimination in all its forms",
"Education is the key to opportunity and we need to invest in our schools and teachers",
"Immigration reform should reflect our values of compassion while maintaining security",
"Women's reproductive rights must be protected as a fundamental freedom"
)
republican_economy <- c(
"Lower taxes and reduced regulations will unleash economic growth and job creation",
"Free market solutions drive innovation and prosperity better than government intervention",
"We must reduce the national debt and ensure fiscal responsibility for future generations",
"Energy independence through domestic production will strengthen our economy and security",
"Small government and individual liberty are the foundations of American prosperity"
)
republican_security <- c(
"National security must remain our top priority in an increasingly dangerous world",
"We need strong borders to protect American citizens and maintain law and order",
"Supporting our military and veterans is essential to defending freedom",
"Law and order must be restored in our communities through supporting police",
"Second Amendment rights are fundamental to American liberty and must be protected"
)
bipartisan_texts <- c(
"We must work together across party lines to address the opioid crisis",
"Infrastructure investment will create jobs and improve quality of life nationwide",
"Veterans deserve our full support and quality healthcare services",
"American innovation and competitiveness depend on STEM education",
"Rural communities need better broadband access for economic development"
)
# Create balanced dataset
set.seed(123)
# Calculate how many of each type
n_dem_econ <- round(n_docs * 0.20)
n_dem_soc <- round(n_docs * 0.20)
n_rep_econ <- round(n_docs * 0.20)
n_rep_sec <- round(n_docs * 0.20)
n_bipart <- n_docs - (n_dem_econ + n_dem_soc + n_rep_econ + n_rep_sec)
corpus <- tibble(
doc_id = paste0("doc_", sprintf("%03d", 1:n_docs)),
text = c(
sample(democrat_economy, n_dem_econ, replace = TRUE),
sample(democrat_social, n_dem_soc, replace = TRUE),
sample(republican_economy, n_rep_econ, replace = TRUE),
sample(republican_security, n_rep_sec, replace = TRUE),
sample(bipartisan_texts, n_bipart, replace = TRUE)
),
party = c(
rep("Democrat", n_dem_econ + n_dem_soc),
rep("Republican", n_rep_econ + n_rep_sec),
rep("Bipartisan", n_bipart)
),
topic = c(
rep("Economy", n_dem_econ),
rep("Social Policy", n_dem_soc),
rep("Economy", n_rep_econ),
rep("Security", n_rep_sec),
rep("Bipartisan", n_bipart)
),
year = sample(2020:2024, n_docs, replace = TRUE),
speaker_type = sample(c("Senator", "Representative", "Governor", "President"),
n_docs, replace = TRUE,
prob = c(0.4, 0.4, 0.15, 0.05))
)
return(corpus)
}
sample_corpus <- generate_sample_corpus(60)
write_csv(sample_corpus, "sample_political_corpus.csv")
cat("Generated sample corpus with", nrow(sample_corpus), "documents\n")
cat("Saved to: sample_political_corpus.csv\n\n")
# Display summary statistics
cat("Corpus Summary:\n")
cat("==============\n")
table(sample_corpus$party) %>% print()
cat("\n")
table(sample_corpus$topic) %>% print()
cat("\n")
table(sample_corpus$year) %>% print()
# Show first few rows
cat("\nFirst 5 documents:\n")
print(head(sample_corpus, 5))
knitr::opts_chunk$set(
echo = TRUE,
warning = FALSE,
message = FALSE,
fig.width = 10,
fig.height = 6
)
# Run this once to install required packages
install.packages(c(
"tidytext",      # Tidy text mining
"quanteda",      # Quantitative text analysis
"quanteda.textstat",
"stringr",       # String manipulation
"dplyr",         # Data manipulation
"tidyr",         # Data tidying
"ggplot2",       # Visualization
"wordcloud",     # Word clouds
"ggraph",        # Network graphs
"igraph",        # Network analysis
"scales",         # Plot scaling
"reshape2",
"textshape"
))
knitr::opts_chunk$set(
echo = TRUE,
warning = FALSE,
message = FALSE,
fig.width = 10,
fig.height = 6
)
install.packages(c(
"tidytext",      # Tidy text mining
"quanteda",      # Quantitative text analysis
"quanteda.textstat",
"stringr",       # String manipulation
"dplyr",         # Data manipulation
"tidyr",         # Data tidying
"ggplot2",       # Visualization
"wordcloud",     # Word clouds
"ggraph",        # Network graphs
"igraph",        # Network analysis
"scales",         # Plot scaling
"reshape2",
"textshape"
))
library(tidytext)
library(quanteda)
library(quanteda.textstats)
library(stringr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(wordcloud)
library(ggraph)
library(igraph)
library(scales)
library(textshape)
# Set a theme for consistent visualizations
theme_set(theme_minimal(base_size = 12))
# Sample State of the Union excerpts (simplified for demonstration)
political_texts <- tibble(
doc_id = paste0("doc_", 1:8),
text = c(
"We must ensure economic security for all Americans through job creation and fair wages",
"National security remains our top priority as we face threats from abroad",
"Climate change poses an existential threat requiring immediate action and international cooperation",
"Healthcare reform will expand access while reducing costs for working families",
"Immigration policy must balance security concerns with our values of compassion and opportunity",
"Education investment is key to economic competitiveness and social mobility",
"We will strengthen democracy by protecting voting rights and reducing money in politics",
"Tax reform should ensure the wealthy pay their fair share while supporting middle class families"
),
party = c("Democrat", "Republican", "Democrat", "Democrat",
"Republican", "Democrat", "Democrat", "Republican"),
topic = c("Economy", "Security", "Environment", "Healthcare",
"Immigration", "Education", "Democracy", "Economy")
)
# Display the data
political_texts
# Function for basic text cleaning
clean_text <- function(text) {
text %>%
str_to_lower() %>%                    # Convert to lowercase
str_replace_all("[[:punct:]]", " ") %>%  # Remove punctuation
str_replace_all("\\s+", " ") %>%         # Normalize whitespace
str_trim()                               # Trim leading/trailing spaces
}
# Apply cleaning
political_texts <- political_texts %>%
mutate(text_clean = clean_text(text))
# Compare original vs. cleaned
political_texts %>%
select(doc_id, text, text_clean) %>%
head(3)
# Sample State of the Union excerpts (simplified for demonstration)
political_texts <- tibble(
doc_id = paste0("doc_", 1:8),
text = c(
" We must ensure!! economic security for all Americans through job creation and fair wages",
"National security remains our top priority as we face threats from abroad",
"Climate change poses an existential threat requiring immediate action and international cooperation",
"Healthcare reform will expand access while reducing costs for working families",
"Immigration policy must balance security concerns with our values of compassion and opportunity",
"Education investment is key to economic competitiveness and social mobility",
"We will strengthen democracy by protecting voting rights and reducing money in politics",
"Tax reform should ensure the wealthy pay their fair share while supporting middle class families"
),
party = c("Democrat", "Republican", "Democrat", "Democrat",
"Republican", "Democrat", "Democrat", "Republican"),
topic = c("Economy", "Security", "Environment", "Healthcare",
"Immigration", "Education", "Democracy", "Economy")
)
# Display the data
political_texts
# Function for basic text cleaning
clean_text <- function(text) {
text %>%
str_to_lower() %>%                    # Convert to lowercase
str_replace_all("[[:punct:]]", " ") %>%  # Remove punctuation
str_replace_all("\\s+", " ") %>%         # Normalize whitespace
str_trim()                               # Trim leading/trailing spaces
}
# Apply cleaning
political_texts <- political_texts %>%
mutate(text_clean = clean_text(text))
# Compare original vs. cleaned
political_texts %>%
select(doc_id, text, text_clean) %>%
head(3)
# Sample State of the Union excerpts (simplified for demonstration)
political_texts <- tibble(
doc_id = paste0("doc_", 1:8),
text = c(
" We must ensure economic security for all Americans through job creation and fair wages!",
"National security remains our top priority as we face threats from abroad",
"Climate change poses an existential threat requiring immediate action and international cooperation",
"Healthcare reform will expand access while reducing costs for working families",
"Immigration policy must balance security concerns with our values of compassion and opportunity",
"Education investment is key to economic competitiveness and social mobility",
"We will strengthen democracy by protecting voting rights and reducing money in politics",
"Tax reform should ensure the wealthy pay their fair share while supporting middle class families"
),
party = c("Democrat", "Republican", "Democrat", "Democrat",
"Republican", "Democrat", "Democrat", "Republican"),
topic = c("Economy", "Security", "Environment", "Healthcare",
"Immigration", "Education", "Democracy", "Economy")
)
# Display the data
political_texts
# Function for basic text cleaning
clean_text <- function(text) {
text %>%
str_to_lower() %>%                    # Convert to lowercase
str_replace_all("[[:punct:]]", " ") %>%  # Remove punctuation
str_replace_all("\\s+", " ") %>%         # Normalize whitespace
str_trim()                               # Trim leading/trailing spaces
}
# Apply cleaning
political_texts <- political_texts %>%
mutate(text_clean = clean_text(text))
# Compare original vs. cleaned
political_texts %>%
select(doc_id, text, text_clean) %>%
head(3)
# Function for basic text cleaning
clean_text <- function(text) {
text %>%
str_to_lower() %>%                    # Convert to lowercase
str_replace_all("!", " ") %>%  # Remove punctuation
str_replace_all("\\s+", " ") %>%         # Normalize whitespace
str_trim()                               # Trim leading/trailing spaces
}
# Apply cleaning
political_texts <- political_texts %>%
mutate(text_clean = clean_text(text))
# Compare original vs. cleaned
political_texts %>%
select(doc_id, text, text_clean) %>%
head(3)
# Sample State of the Union excerpts (simplified for demonstration)
political_texts <- tibble(
doc_id = paste0("doc_", 1:8),
text = c(
"We must ensure economic security for all Americans through job creation and fair wages",
"National security remains our top priority as we face threats from abroad",
"Climate change poses an existential threat requiring immediate action and international cooperation",
"Healthcare reform will expand access while reducing costs for working families",
"Immigration policy must balance security concerns with our values of compassion and opportunity",
"Education investment is key to economic competitiveness and social mobility",
"We will strengthen democracy by protecting voting rights and reducing money in politics",
"Tax reform should ensure the wealthy pay their fair share while supporting middle class families"
),
party = c("Democrat", "Republican", "Democrat", "Democrat",
"Republican", "Democrat", "Democrat", "Republican"),
topic = c("Economy", "Security", "Environment", "Healthcare",
"Immigration", "Education", "Democracy", "Economy")
)
# Display the data
political_texts
# Function for basic text cleaning
clean_text <- function(text) {
text %>%
str_to_lower() %>%                    # Convert to lowercase
str_replace_all("[[:punct:]]", " ") %>%  # Remove punctuation
str_replace_all("\\s+", " ") %>%         # Normalize whitespace
str_trim()                               # Trim leading/trailing spaces
}
# Apply cleaning
political_texts <- political_texts %>%
mutate(text_clean = clean_text(text))
# Compare original vs. cleaned
political_texts %>%
select(doc_id, text, text_clean) %>%
head(3)
# Tokenize into unigrams
unigrams <- political_texts %>%
unnest_tokens(word, text_clean) %>%
select(doc_id, party, topic, word)
# View first few tokens
head(unigrams, 10)
# Count total tokens
cat("Total tokens:", nrow(unigrams), "\n")
cat("Unique tokens:", n_distinct(unigrams$word), "\n")
# Load standard English stop words
data("stop_words")
# Remove stop words
unigrams_clean <- unigrams %>%
anti_join(stop_words, by = "word")
cat("Tokens after stop word removal:", nrow(unigrams_clean), "\n")
cat("Unique tokens:", n_distinct(unigrams_clean$word), "\n")
# Calculate word frequencies
word_freq <- unigrams_clean %>%
count(word, sort = TRUE)
# Top 20 words
top_words <- word_freq %>%
head(20)
print(top_words)
# Visualize top words
ggplot(top_words, aes(x = reorder(word, n), y = n)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(
title = "Top 20 Most Frequent Words",
subtitle = "After removing stop words",
x = NULL,
y = "Frequency"
)
# Word frequencies by party
party_words <- unigrams_clean %>%
count(party, word, sort = TRUE) %>%
group_by(party) %>%
slice_max(n, n = 10) %>%
ungroup()
# Visualize
ggplot(party_words, aes(x = reorder(word, n), y = n, fill = party)) +
geom_col(show.legend = FALSE) +
facet_wrap(~party, scales = "free") +
coord_flip() +
scale_fill_manual(values = c("Democrat" = "#4575b4", "Republican" = "#d73027")) +
labs(
title = "Top Words by Political Party",
x = NULL,
y = "Frequency"
)
# Create bigrams
bigrams <- political_texts %>%
unnest_tokens(bigram, text_clean, token = "ngrams", n = 2) %>%
select(doc_id, party, topic, bigram)
head(bigrams, 10)
# Count bigram frequencies
bigram_freq <- bigrams %>%
count(bigram, sort = TRUE)
print(head(bigram_freq, 15))
# Separate bigrams into two columns
bigrams_separated <- bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
# Filter out stop words
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word)
# Count filtered bigrams
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
# Reunite for display
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ") %>%
count(bigram, sort = TRUE)
print(head(bigrams_united, 15))
# Prepare network data (minimum frequency threshold)
bigram_graph <- bigram_counts %>%
filter(n >= 2) %>%  # Adjust threshold as needed
graph_from_data_frame()
# Set seed for reproducible layout
set.seed(42)
# Create network plot
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n),
edge_colour = "steelblue",
show.legend = FALSE) +
geom_node_point(size = 4, color = "darkred") +
geom_node_text(aes(label = name),
vjust = 1.5,
hjust = 1,
size = 3.5) +
labs(title = "Bigram Network Graph",
subtitle = "Connections between frequently co-occurring words")
# Create corpus object
corp <- corpus(political_texts, text_field = "text_clean")
# Create document-term matrix
dtm <- corp %>%
tokens(remove_punct = TRUE) %>%
tokens_remove(stop_words$word) %>%
dfm()
# Examine the DTM
print(dtm)
# View first few rows and columns
head(convert(dtm, to = "data.frame"), 8)
# Matrix dimensions
cat("Documents:", ndoc(dtm), "\n")
cat("Features (unique terms):", nfeat(dtm), "\n")
cat("Sparsity:", sparsity(dtm) %>% round(3), "\n")
# Most frequent features
topfeatures(dtm, 20)
# Calculate cosine similarity between documents
doc_similarity <- textstat_simil(dtm, method = "cosine")
# Convert to matrix for viewing
sim_matrix <- as.matrix(doc_similarity)
rownames(sim_matrix) <- paste0("Doc", 1:8)
colnames(sim_matrix) <- paste0("Doc", 1:8)
# Display similarity matrix
round(sim_matrix, 3)
# Visualize as heatmap
library(reshape2)
melted_sim <- melt(sim_matrix)
ggplot(melted_sim, aes(x = Var1, y = Var2, fill = value)) +
geom_tile() +
scale_fill_gradient2(low = "white", high = "darkblue",
mid = "lightblue", midpoint = 0.5,
limits = c(0, 1)) +
geom_text(aes(label = round(value, 2)), size = 3) +
labs(title = "Document Similarity Matrix",
subtitle = "Cosine similarity based on word frequencies",
x = NULL, y = NULL, fill = "Similarity") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Calculate TF-IDF using tidytext
tfidf_scores <- unigrams_clean %>%
count(doc_id, word) %>%
bind_tf_idf(word, doc_id, n) %>%
arrange(desc(tf_idf))
# View highest TF-IDF scores
head(tfidf_scores, 20)
# Top TF-IDF terms per document
top_tfidf <- tfidf_scores %>%
group_by(doc_id) %>%
slice_max(tf_idf, n = 5) %>%
ungroup()
# Visualize
ggplot(top_tfidf, aes(x = reorder(word, tf_idf), y = tf_idf, fill = doc_id)) +
geom_col(show.legend = FALSE) +
facet_wrap(~doc_id, ncol = 2, scales = "free") +
coord_flip() +
labs(
title = "Top TF-IDF Terms by Document",
subtitle = "Words most distinctive to each document",
x = NULL,
y = "TF-IDF Score"
)
