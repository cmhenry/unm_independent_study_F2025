# Define word pairs to compare
word_pairs <- tribble(
~word1, ~word2, ~expected,
"american", "prosperity", "High",
"healthcare", "quality", "Medium",
"environmental", "tech", "Low",
"government", "nations", "High",
"economy", "healthcare", "Medium",
"digital", "tech", "High"
)
# Compute similarities
word_pairs <- word_pairs %>%
rowwise() %>%
mutate(
similarity = {
if (word1 %in% rownames(embeddings) && word2 %in% rownames(embeddings)) {
vec1 <- embeddings[word1, ]
vec2 <- embeddings[word2, ]
sum(vec1 * vec2) / (sqrt(sum(vec1^2)) * sqrt(sum(vec2^2)))
} else {
NA
}
}
) %>%
ungroup()
print(word_pairs)
# Visualize
word_pairs %>%
filter(!is.na(similarity)) %>%
mutate(pair = paste(word1, "â†”", word2)) %>%
ggplot(aes(x = reorder(pair, similarity), y = similarity, fill = expected)) +
geom_col() +
coord_flip() +
scale_fill_manual(values = c("High" = "#27ae60", "Medium" = "#f39c12", "Low" = "#e74c3c")) +
labs(
title = "Semantic Similarity Between Word Pairs",
subtitle = "Word2Vec embeddings capture expected relationships",
x = NULL,
y = "Cosine Similarity",
fill = "Expected\nSimilarity"
)
# Helper function for analogies
word_analogy <- function(model, positive, negative, top_n = 5) {
# Check if all words are in vocabulary
all_words <- c(positive, negative)
missing <- all_words[!all_words %in% rownames(embeddings)]
if (length(missing) > 0) {
cat("Missing words:", paste(missing, collapse = ", "), "\n")
return(NULL)
}
# Compute: positive[1] - negative[1] + positive[2]
result_vec <- embeddings[positive[1], ] - embeddings[negative[1], ] + embeddings[positive[2], ]
# Find nearest words
similarities <- embeddings %*% result_vec /
(sqrt(rowSums(embeddings^2)) * sqrt(sum(result_vec^2)))
# Remove input words and get top results
similarities <- similarities[!rownames(embeddings) %in% all_words]
top_indices <- order(similarities, decreasing = TRUE)[1:top_n]
tibble(
word = rownames(embeddings)[as.numeric(names(similarities)[top_indices])],
similarity = as.numeric(similarities[top_indices])
)
}
# Try political analogies
cat("democracy - freedom + control = ?\n")
word_analogy(w2v_model, c("democracy", "control"), c("freedom"), top_n = 5)
cat("\ngovernment - elections + military = ?\n")
word_analogy(w2v_model, c("government", "military"), c("elections"), top_n = 5)
cat("\neconomic - growth + decline = ?\n")
word_analogy(w2v_model, c("economic", "decline"), c("growth"), top_n = 5)
rownames(embedding)
rownames(embeddings)
# Helper function for analogies
word_analogy <- function(model, positive, negative, top_n = 5) {
# Check if all words are in vocabulary
all_words <- c(positive, negative)
missing <- all_words[!all_words %in% rownames(embeddings)]
if (length(missing) > 0) {
cat("Missing words:", paste(missing, collapse = ", "), "\n")
return(NULL)
}
# Compute: positive[1] - negative[1] + positive[2]
result_vec <- embeddings[positive[1], ] - embeddings[negative[1], ] + embeddings[positive[2], ]
# Find nearest words
similarities <- embeddings %*% result_vec /
(sqrt(rowSums(embeddings^2)) * sqrt(sum(result_vec^2)))
# Remove input words and get top results
similarities <- similarities[!rownames(embeddings) %in% all_words]
top_indices <- order(similarities, decreasing = TRUE)[1:top_n]
tibble(
word = rownames(embeddings)[as.numeric(names(similarities)[top_indices])],
similarity = as.numeric(similarities[top_indices])
)
}
# Try political analogies
cat("educational - students + teacher = ?\n")
word_analogy(w2v_model, c("educational", "students"), c("teacher"), top_n = 5)
# Helper function for analogies
word_analogy <- function(model, positive, negative, top_n = 5) {
# Check if all words are in vocabulary
all_words <- c(positive, negative)
missing <- all_words[!all_words %in% rownames(embeddings)]
if (length(missing) > 0) {
cat("Missing words:", paste(missing, collapse = ", "), "\n")
return(NULL)
}
# Compute: positive[1] - negative[1] + positive[2]
result_vec <- embeddings[positive[1], ] - embeddings[negative[1], ] + embeddings[positive[2], ]
# Find nearest words
similarities <- embeddings %*% result_vec /
(sqrt(rowSums(embeddings^2)) * sqrt(sum(result_vec^2)))
# Remove input words and get top results
similarities <- similarities[!rownames(embeddings) %in% all_words]
top_indices <- order(similarities, decreasing = TRUE)[1:top_n]
tibble(
word = rownames(embeddings)[as.numeric(names(similarities)[top_indices])],
similarity = as.numeric(similarities[top_indices])
)
}
# Try political analogies
cat("educational - students + teacher = ?\n")
word_analogy(w2v_model, c("democracy", "control"), c("freedom"), top_n = 5)
cat("\ngovernment - elections + military = ?\n")
word_analogy(w2v_model, c("government", "military"), c("elections"), top_n = 5)
cat("\neconomic - growth + decline = ?\n")
word_analogy(w2v_model, c("economic", "decline"), c("growth"), top_n = 5)
word_analogy(w2v_model, c("democracy", "control"), c("freedom"), top_n = 5)
word_analogy(w2v_model, c("economic", "decline"), c("growth"), top_n = 5)
rownames(embeddings)
word_analogy(w2v_model, c("economic", "regulation"), c("growth"), top_n = 5)
positive <- c("economic","regulation")
negative <- "growth"
all_words <- c(positive, negative)
missing <- all_words[!all_words %in% rownames(embeddings)]
missing
result_vec <- embeddings[positive[1], ] - embeddings[negative[1], ] + embeddings[positive[2], ]
result_vec
similarities <- embeddings %*% result_vec /
(sqrt(rowSums(embeddings^2)) * sqrt(sum(result_vec^2)))
similarities
similarities <- similarities[!rownames(embeddings) %in% all_words]
top_indices <- order(similarities, decreasing = TRUE)[1:top_n]
top_indices <- order(similarities, decreasing = TRUE)[1:5]
top_indices
names(similarities)[top_indices]
names(similarities)
similarities
similarities <- embeddings %*% result_vec /
(sqrt(rowSums(embeddings^2)) * sqrt(sum(result_vec^2)))
similarities
similarities[!rownames(embeddings) %in% all_words]
rownames(embeddings)
View(similarities)
similarities[1]
similarities$V1
View(similarities)
rownames(similarities)
rownames(similarities[!rownames(embeddings) %in% all_words])
similarities <- similarities[!rownames(embeddings) %in% all_words]
rownames(smilarities)
rownames(similarities)
similarities <- embeddings %*% result_vec /
(sqrt(rowSums(embeddings^2)) * sqrt(sum(result_vec^2)))
top_indices <- order(similarities, decreasing = TRUE)[1:5]
top_indices
tibble(
word = rownames(embeddings)[as.numeric(names(similarities)[top_indices])],
similarity = as.numeric(similarities[top_indices])
)
names(similarities)
rownames(similarities)
tibble(
word = rownames(embeddings)[as.numeric(rownames(similarities)[top_indices])],
similarity = as.numeric(similarities[top_indices])
)
tibble(
word = rownames(embeddings)[as.numeric(similarities[top_indices])],
similarity = as.numeric(similarities[top_indices])
)
# Define analogies to test
analogies <- tribble(
~word1, ~word2, ~word3, ~relation,
"democracy", "freedom", "autocracy", "democracy:freedom :: autocracy:?",
"president", "executive", "congress", "president:executive :: congress:?",
"liberal", "progressive", "conservative", "liberal:progressive :: conservative:?",
"economy", "fiscal", "healthcare", "economy:fiscal :: healthcare:?"
)
# Compute analogies
analogy_results <- analogies %>%
rowwise() %>%
mutate(
result = {
res <- word_analogy(w2v_model, c(word1, word3), c(word2), top_n = 3)
if (!is.null(res)) {
paste(res$word[1:min(3, nrow(res))], collapse = ", ")
} else {
"N/A"
}
}
) %>%
ungroup()
print(analogy_results)
# Select interesting political words
political_words <- c(
"democracy", "autocracy", "freedom", "control",
"liberal", "conservative", "progressive", "moderate",
"economy", "healthcare", "security", "education",
"government", "regime", "state", "nation",
"election", "voting", "campaign", "policy"
)
# Filter to words in vocabulary
political_words <- political_words[political_words %in% rownames(embeddings)]
# Get embeddings for these words
word_embeddings <- embeddings[political_words, ]
# Perform PCA
pca_result <- prcomp(word_embeddings, center = TRUE, scale. = TRUE)
# Create visualization data
pca_data <- tibble(
word = political_words,
PC1 = pca_result$x[, 1],
PC2 = pca_result$x[, 2]
)
# Plot
ggplot(pca_data, aes(x = PC1, y = PC2, label = word)) +
geom_point(size = 3, alpha = 0.7, color = "steelblue") +
geom_text_repel(size = 3.5, max.overlaps = 20) +
labs(
title = "Political Word Embeddings in 2D Space (PCA)",
subtitle = "Similar words cluster together",
x = paste0("PC1 (", round(summary(pca_result)$importance[2, 1] * 100, 1), "% variance)"),
y = paste0("PC2 (", round(summary(pca_result)$importance[2, 2] * 100, 1), "% variance)")
)
library(Rtsne)
# t-SNE requires more data points - let's use top 50 words
top_words <- head(rownames(embeddings), 50)
top_embeddings <- embeddings[top_words, ]
# Run t-SNE
tsne_result <- Rtsne(
top_embeddings,
dims = 2,
perplexity = 10,  # Lower for small datasets
max_iter = 1000,
check_duplicates = FALSE
)
# Create visualization data
tsne_data <- tibble(
word = top_words,
tsne1 = tsne_result$Y[, 1],
tsne2 = tsne_result$Y[, 2]
)
# Plot
ggplot(tsne_data, aes(x = tsne1, y = tsne2, label = word)) +
geom_point(size = 3, alpha = 0.7, color = "darkred") +
geom_text_repel(size = 3, max.overlaps = 20) +
labs(
title = "Word Embeddings in 2D Space (t-SNE)",
subtitle = "Non-linear projection preserves local neighborhoods",
x = "t-SNE Dimension 1",
y = "t-SNE Dimension 2"
)
# GloVe embeddings must be downloaded separately
# Download from: https://nlp.stanford.edu/projects/glove/
# We'll use glove.6B.100d.txt (100-dimensional, trained on 6B tokens)
# Load GloVe embeddings (if you have them)
glove_file <- "path/to/glove.6B.100d.txt"
if (file.exists(glove_file)) {
glove <- read.table(
glove_file,
quote = "",
comment.char = "",
stringsAsFactors = FALSE
)
# First column is word, rest are embedding values
glove_words <- glove[, 1]
glove_embeddings <- as.matrix(glove[, -1])
rownames(glove_embeddings) <- glove_words
cat("Loaded GloVe embeddings:", nrow(glove_embeddings), "words\n")
}
# Compare word similarities in both embedding spaces
test_word <- "democracy"
# Custom Word2Vec
cat("Custom Word2Vec - similar to", test_word, ":\n")
predict(w2v_model, c(test_word), type = "nearest", top_n = 5)
# Compare word similarities in both embedding spaces
test_word <- "economy"
# Custom Word2Vec
cat("Custom Word2Vec - similar to", test_word, ":\n")
predict(w2v_model, c(test_word), type = "nearest", top_n = 5)
# Pre-trained GloVe (if loaded)
if (exists("glove_embeddings")) {
if (test_word %in% rownames(glove_embeddings)) {
test_vec <- glove_embeddings[test_word, ]
similarities <- glove_embeddings %*% test_vec /
(sqrt(rowSums(glove_embeddings^2)) * sqrt(sum(test_vec^2)))
top_indices <- order(similarities, decreasing = TRUE)[2:6]  # Skip self
cat("\nPre-trained GloVe - similar to", test_word, ":\n")
print(tibble(
word = rownames(glove_embeddings)[top_indices],
similarity = as.numeric(similarities[top_indices])
))
}
}
# GloVe embeddings must be downloaded separately
# Download from: https://nlp.stanford.edu/projects/glove/
# We'll use glove.6B.100d.txt (100-dimensional, trained on 6B tokens)
# Load GloVe embeddings (if you have them)
glove_file <- "wiki_giga_2024_100_MFT20_vectors_seed_2024_alpha_0.75_eta_0.05.050_combined"
if (file.exists(glove_file)) {
glove <- read.table(
glove_file,
quote = "",
comment.char = "",
stringsAsFactors = FALSE
)
# First column is word, rest are embedding values
glove_words <- glove[, 1]
glove_embeddings <- as.matrix(glove[, -1])
rownames(glove_embeddings) <- glove_words
cat("Loaded GloVe embeddings:", nrow(glove_embeddings), "words\n")
}
getwd()
setwd("~/Projects/unm_independent_study_F2025/week2")
# GloVe embeddings must be downloaded separately
# Download from: https://nlp.stanford.edu/projects/glove/
# We'll use glove.6B.100d.txt (100-dimensional, trained on 6B tokens)
# Load GloVe embeddings (if you have them)
glove_file <- "wiki_giga_2024_100_MFT20_vectors_seed_2024_alpha_0.75_eta_0.05.050_combined"
if (file.exists(glove_file)) {
glove <- read.table(
glove_file,
quote = "",
comment.char = "",
stringsAsFactors = FALSE
)
# First column is word, rest are embedding values
glove_words <- glove[, 1]
glove_embeddings <- as.matrix(glove[, -1])
rownames(glove_embeddings) <- glove_words
cat("Loaded GloVe embeddings:", nrow(glove_embeddings), "words\n")
}
# GloVe embeddings must be downloaded separately
# Download from: https://nlp.stanford.edu/projects/glove/
# We'll use glove.6B.100d.txt (100-dimensional, trained on 6B tokens)
# Load GloVe embeddings (if you have them)
glove_file <- "wiki_giga_2024_100_MFT20_vectors_seed_2024_alpha_0.75_eta_0.05.050_combined.txt"
if (file.exists(glove_file)) {
glove <- read.table(
glove_file,
quote = "",
comment.char = "",
stringsAsFactors = FALSE
)
# First column is word, rest are embedding values
glove_words <- glove[, 1]
glove_embeddings <- as.matrix(glove[, -1])
rownames(glove_embeddings) <- glove_words
cat("Loaded GloVe embeddings:", nrow(glove_embeddings), "words\n")
}
# Compare word similarities in both embedding spaces
test_word <- "economy"
# Custom Word2Vec
cat("Custom Word2Vec - similar to", test_word, ":\n")
predict(w2v_model, c(test_word), type = "nearest", top_n = 5)
# Pre-trained GloVe (if loaded)
if (exists("glove_embeddings")) {
if (test_word %in% rownames(glove_embeddings)) {
test_vec <- glove_embeddings[test_word, ]
similarities <- glove_embeddings %*% test_vec /
(sqrt(rowSums(glove_embeddings^2)) * sqrt(sum(test_vec^2)))
top_indices <- order(similarities, decreasing = TRUE)[2:6]  # Skip self
cat("\nPre-trained GloVe - similar to", test_word, ":\n")
print(tibble(
word = rownames(glove_embeddings)[top_indices],
similarity = as.numeric(similarities[top_indices])
))
}
}
# Function to compute document embedding (average of word vectors)
get_doc_embedding <- function(text, word_embeddings, remove_stopwords = TRUE) {
# Tokenize
words <- text %>%
str_to_lower() %>%
str_split("\\s+") %>%
unlist()
# Optionally remove stop words
if (remove_stopwords) {
words <- words[!words %in% stop_words$word]
}
# Get vectors for words in vocabulary
words_in_vocab <- words[words %in% rownames(word_embeddings)]
if (length(words_in_vocab) == 0) {
return(rep(0, ncol(word_embeddings)))
}
# Average word vectors
word_vecs <- word_embeddings[words_in_vocab, , drop = FALSE]
colMeans(word_vecs)
}
# Compute document embeddings for our corpus
doc_embeddings <- lapply(corpus_clean, function(text) {
get_doc_embedding(text, embeddings, remove_stopwords = TRUE)
})
doc_embeddings_matrix <- do.call(rbind, doc_embeddings)
rownames(doc_embeddings_matrix) <- paste0("doc_", 1:nrow(doc_embeddings_matrix))
cat("Document embeddings:", dim(doc_embeddings_matrix), "\n")
# Cosine similarity between documents
library(proxy)
doc_sim_embeddings <- as.matrix(simil(doc_embeddings_matrix, method = "cosine"))
# Compare first 10 documents
doc_sim_subset <- doc_sim_embeddings[1:10, 1:10]
# Visualize as heatmap
library(reshape2)
melted_sim <- melt(doc_sim_subset)
ggplot(melted_sim, aes(x = Var1, y = Var2, fill = value)) +
geom_tile() +
scale_fill_gradient2(
low = "white",
mid = "lightblue",
high = "darkblue",
midpoint = 0.5,
limits = c(0, 1)
) +
geom_text(aes(label = round(value, 2)), size = 3) +
labs(
title = "Document Similarity Matrix (Embedding-Based)",
subtitle = "Based on averaged word embeddings",
x = "Document",
y = "Document",
fill = "Cosine\nSimilarity"
) +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Create BoW document-term matrix
corp <- corpus(corpus_clean[1:10])
dtm_bow <- corp %>%
tokens() %>%
tokens_remove(stop_words$word) %>%
dfm()
# BoW similarity
doc_sim_bow <- as.matrix(textstat_simil(dtm_bow, method = "cosine"))
library(quanteda.textstats)
# Create BoW document-term matrix
corp <- corpus(corpus_clean[1:10])
dtm_bow <- corp %>%
tokens() %>%
tokens_remove(stop_words$word) %>%
dfm()
# BoW similarity
doc_sim_bow <- as.matrix(textstat_simil(dtm_bow, method = "cosine"))
# Compare similarities
comparison <- tibble(
doc_pair = "doc_1 vs doc_2",
bow_similarity = doc_sim_bow[1, 2],
embedding_similarity = doc_sim_embeddings[1, 2],
difference = embedding_similarity - bow_similarity
)
# Add more comparisons
for (i in 1:5) {
for (j in (i+1):6) {
if (j <= 10) {
comparison <- comparison %>%
add_row(
doc_pair = paste0("doc_", i, " vs doc_", j),
bow_similarity = doc_sim_bow[i, j],
embedding_similarity = doc_sim_embeddings[i, j],
difference = embedding_similarity - bow_similarity
)
}
}
}
print(comparison)
# Visualize comparison
comparison %>%
pivot_longer(cols = c(bow_similarity, embedding_similarity),
names_to = "method",
values_to = "similarity") %>%
ggplot(aes(x = doc_pair, y = similarity, fill = method)) +
geom_col(position = "dodge") +
coord_flip() +
scale_fill_manual(
values = c("bow_similarity" = "#e74c3c", "embedding_similarity" = "#3498db"),
labels = c("Bag of Words", "Word Embeddings")
) +
labs(
title = "Document Similarity: BoW vs. Embeddings",
subtitle = "Embeddings capture semantic similarity beyond exact word matches",
x = NULL,
y = "Cosine Similarity",
fill = "Method"
)
# Embeddings are particularly useful when:
# 1. You need to find semantically similar documents
similar_docs <- which(doc_sim_embeddings[1, ] > 0.8)
cat("Documents similar to doc_1:", similar_docs, "\n")
# 2. You want to capture synonyms and related concepts
predict(w2v_model, c("economy"), type = "nearest", top_n = 10)
# 3. You're working with varied vocabulary
# (embeddings handle "nation", "country", "state" as similar)
# 4. You need to measure concept relationships
# (e.g., how "democracy" relates to "freedom" vs. "control")
# Embeddings need sufficient data to learn relationships
cat("Our corpus size:", length(corpus_clean), "documents\n")
cat("Vocabulary size:", length(w2v_model), "words\n")
cat("\n")
cat("Rodriguez & Spirling recommend:\n")
cat("- Minimum 500 documents for domain-specific embeddings\n")
cat("- Preferably 5,000+ documents for reliable relationships\n")
cat("- Use pre-trained embeddings if your corpus is smaller\n")
# Example: "liberal" means different things in different contexts
# - US politics: left-wing
# - European politics: free-market
# - Historical texts: freedom-oriented
# Pre-trained embeddings may not capture your specific domain usage
# Solution: Train on domain-specific corpus OR fine-tune pre-trained embeddings
